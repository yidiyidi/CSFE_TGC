{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b0f2bb0",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed4d96e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pywt\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "from scipy import spatial\n",
    "import sklearn\n",
    "from PIL import Image\n",
    "import pickle as pk\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.autograd import Variable as V\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e42d6",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb1f9432",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#upload pickle\n",
    "f = open('D:/论文/work/2022.07/0721-CNN and loss function simulation/RML2016.10a_dict.pkl','rb')\n",
    "signal_data = pk.load(f, encoding = 'latin1')\n",
    "Y = ['QPSK', 'PAM4', 'AM-DSB', 'GFSK', 'QAM64', 'AM-SSB', '8PSK', 'QAM16', 'WBFM', 'CPFSK', 'BPSK']\n",
    "SNR = np.arange(-20, 20, 2)\n",
    "X = np.zeros([11, 20, 1000, 2, 128])\n",
    "ii = 0\n",
    "for i in Y:\n",
    "    jj = 0\n",
    "    for j in SNR:\n",
    "        X[ii, jj, :, :, :] = signal_data[(i, j)]\n",
    "        jj += 1\n",
    "    ii += 1\n",
    "# plot_x = range(1, 129)\n",
    "# plt.plot(plot_x, X[0,0,0,0, :])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886978da",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Select a type of snr\n",
    "def select_snr(x, snr):\n",
    "    X_choosed = x[:, snr, :, :, :]\n",
    "    X_choosed = np.reshape(X_choosed, [X_choosed.shape[0], 1000, 2, 128])\n",
    "    return X_choosed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eded96",
   "metadata": {},
   "source": [
    "## Add Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d3348e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def noisy_label(y, noise_ratio):\n",
    "    noise_index = random.sample(list(np.arange(len(y) / 11).astype(int)), int(len(y) * noise_ratio / 11))\n",
    "    tt = np.arange(11)\n",
    "    for k in range(11):\n",
    "        ttt = np.setdiff1d(tt, [k])\n",
    "        for i in noise_index:\n",
    "            y[int(i + k*len(y)/11)] = np.random.choice(ttt, replace = True)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe92cd14",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def decomp_noise(x, snr, noise_ratio, train_ratio):\n",
    "    X_choosed = select_snr(x, snr)\n",
    "    n = 1000\n",
    "    n1 = int(n * train_ratio)\n",
    "    n2 = n - n1\n",
    "    n_noise = int(n1 * noise_ratio)\n",
    "    class_num = X_choosed.shape[0]\n",
    "    Y = np.zeros([1, int(n)], dtype = int)\n",
    "    \n",
    "    #创建空数组\n",
    "    train_index = np.zeros([class_num, n1], dtype = int)\n",
    "    test_index = np.zeros([class_num, n2], dtype = int)\n",
    "    \n",
    "    x_train = np.zeros([class_num, n1, 2, 128])\n",
    "    y_train = np.zeros([class_num, n1])\n",
    "    x_test = np.zeros([class_num, n2, 2, 128])\n",
    "    y_test = np.zeros([class_num, n2])\n",
    "    \n",
    "    #数据集分割\n",
    "    for i in range(1, class_num):\n",
    "        Y = np.concatenate((Y, np.ones([1, n]) * i), 0)\n",
    "    for j in range(class_num):\n",
    "        train_index[j, :] = random.sample(list(np.arange(n)), n1)\n",
    "        test_index[j, :] = np.setdiff1d(np.arange(n), train_index[j, :])\n",
    "\n",
    "        x_train[j, :, :] = X_choosed[j, train_index[j, :], :]\n",
    "        y_train[j, :] = Y[j, train_index[j, :]]\n",
    "        x_test[j, :, :] = X_choosed[j, test_index[j, :], :]\n",
    "        y_test[j, :] = Y[j, test_index[j, :]]\n",
    "\n",
    "    #reshape并对train与test添加错误标签\n",
    "    x_train = np.reshape(x_train, [class_num * n1, 2, 128])\n",
    "    y_train = np.reshape(y_train, [class_num * n1])\n",
    "    y_train = noisy_label(y_train, noise_ratio).astype(int)\n",
    "    \n",
    "    x_test = np.reshape(x_test, [class_num * n2, 2, 128])\n",
    "    y_test = np.reshape(y_test, [class_num * n2])\n",
    "\n",
    "\n",
    "#     train_shuffle = np.random.choice(np.arange(X_choosed.shape[0] * int(1000 * sample_ratio1)), \n",
    "#                                      size=X_choosed.shape[0] * int(1000 * sample_ratio1), replace=False)\n",
    "#     x_train = x_train[train_shuffle, :]\n",
    "#     y_train = y_train[train_shuffle]\n",
    "    \n",
    "   \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0916068",
   "metadata": {},
   "source": [
    "## Achieve the clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db88c8e1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# achieve the clean sample\n",
    "x_train, y_train, x_test, y_test = decomp_noise(X, 10, 0.5, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988900da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3300, 2, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed14805",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.zeros(7700)\n",
    "for i in range(11):\n",
    "    train_label[(700*i) : (700 * (i+1))] = i * np.ones(700)\n",
    "train_label = pd.DataFrame(train_label)\n",
    "train_label.to_csv(\"train_label.csv\")\n",
    "test_label = pd.DataFrame(y_test)\n",
    "test_label.to_csv(\"test_label.csv\")\n",
    "train_noisy_label = pd.DataFrame(y_train)\n",
    "train_noisy_label.to_csv(\"train_noise_label.csv\")\n",
    "origin_train = pd.DataFrame(np.reshape(x_train, [x_train.shape[0], 256]))\n",
    "origin_test = pd.DataFrame(np.reshape(x_test, [x_test.shape[0], 256]))\n",
    "origin_train.to_csv(\"train.csv\")\n",
    "origin_test.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae459f2",
   "metadata": {},
   "source": [
    "## CNN（Save Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e19bae",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#Data Centralization\n",
    "def train_test(x_train, y_train, x_test, y_test):\n",
    "#     train_x, y_train, test_x, y_test = decomp_noise(x, 10, 0.3, 0.6, 0.2) \n",
    "    train_x = np.reshape(x_train, [x_train.shape[0], 1, 2, 128])\n",
    "    test_x = np.reshape(x_test, [x_test.shape[0], 1, 2, 128])\n",
    "    #print(train_x.shape)\n",
    "\n",
    "    train_x = torch.tensor(train_x).to(device)\n",
    "    test_x = torch.tensor(test_x).to(device)\n",
    "    xmean = torch.mean(train_x, dim=0)\n",
    "    xstd = torch.std(train_x, dim=0)\n",
    "    # xmean = torch.unsqueeze(xmean, 0)\n",
    "    # xstd = torch.unsqueeze(xstd, 0)\n",
    "    #print(xmean.shape)\n",
    "\n",
    "    train_x = (train_x - xmean) / (xstd + 1e-7)\n",
    "    test_x = (test_x - xmean) / (xstd + 1e-7)\n",
    "\n",
    "    y_train = torch.tensor(y_train).to(device)\n",
    "    y_test = torch.tensor(y_test).to(device)\n",
    "    \n",
    "    return train_x, test_x, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aec594a",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Dataset Setup\n",
    "\n",
    "train_x, test_x, y_train, y_test = train_test(x_train, y_train, x_test, y_test)\n",
    "batch_size = 60\n",
    "num_epochs = 5\n",
    "num_classes = 11\n",
    "learning_rate = 0.001\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.type(torch.float32)\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index, :, :, :]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x[:, :, :, 0])\n",
    "    \n",
    "\n",
    "train_dataset = MyDataset(train_x, y_train)\n",
    "test_dataset = MyDataset(test_x, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=3300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf22a99f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Network Design\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 50, kernel_size = (1, 8), stride = (1, 1), padding = (0, 2)),\n",
    "          nn.Dropout2d(),\n",
    "          nn.BatchNorm2d(50),\n",
    "          nn.ReLU()\n",
    "          #nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 1))\n",
    "         )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(50, 50, kernel_size=(2, 8), stride=(1, 1), padding=(0, 2)),\n",
    "            nn.Dropout2d(),\n",
    "            nn.BatchNorm2d(50),\n",
    "            nn.ReLU()\n",
    "#             nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 1))\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(6100, 256)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        temp = self.layer3(out)\n",
    "        out = self.layer4(temp)\n",
    "        return out, temp\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e82ebdc6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#训练\n",
    "def train(epoch):\n",
    "    running_loss = 0.0\n",
    "#     if epoch == 100:\n",
    "#         optimizer.param_groups[0]['lr'] /= 2\n",
    "#     if epoch == 200:\n",
    "#         optimizer.param_groups[0]['lr'] /= 2\n",
    "    for batch_idx, data_ in enumerate(train_loader, 0):   # enumerate( , 0)  自动编号 从0开始\n",
    "\n",
    "        inputs, target = data_\n",
    "#         print(inputs)\n",
    "#         print(target)\n",
    "#         break\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward, backward, update\n",
    "        outputs, temp = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 30 == 29:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 30))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "            \n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, temp = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('accuracy on test set: %d %% ' % (100 * correct / total))\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbaad38",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\shiro\\lib\\site-packages\\torch\\nn\\functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    30] loss: 2.508\n",
      "[1,    60] loss: 2.395\n",
      "[1,    90] loss: 2.381\n",
      "[1,   120] loss: 2.375\n",
      "accuracy on test set: 25 % \n",
      "0\n",
      "[2,    30] loss: 2.309\n",
      "[2,    60] loss: 2.313\n",
      "[2,    90] loss: 2.277\n",
      "[2,   120] loss: 2.252\n",
      "accuracy on test set: 35 % \n",
      "25.727272727272727\n",
      "[3,    30] loss: 2.218\n",
      "[3,    60] loss: 2.257\n",
      "[3,    90] loss: 2.201\n",
      "[3,   120] loss: 2.191\n",
      "accuracy on test set: 39 % \n",
      "35.303030303030305\n",
      "[4,    30] loss: 2.171\n",
      "[4,    60] loss: 2.154\n",
      "[4,    90] loss: 2.154\n",
      "[4,   120] loss: 2.133\n",
      "accuracy on test set: 50 % \n",
      "39.24242424242424\n",
      "[5,    30] loss: 2.119\n",
      "[5,    60] loss: 2.120\n",
      "[5,    90] loss: 2.102\n",
      "[5,   120] loss: 2.117\n",
      "accuracy on test set: 57 % \n",
      "50.39393939393939\n",
      "[6,    30] loss: 2.074\n",
      "[6,    60] loss: 2.109\n",
      "[6,    90] loss: 2.028\n",
      "[6,   120] loss: 2.072\n",
      "accuracy on test set: 61 % \n",
      "57.90909090909091\n",
      "[7,    30] loss: 2.034\n",
      "[7,    60] loss: 2.012\n",
      "[7,    90] loss: 1.999\n",
      "[7,   120] loss: 1.996\n",
      "accuracy on test set: 61 % \n",
      "61.57575757575758\n",
      "[8,    30] loss: 1.909\n",
      "[8,    60] loss: 1.913\n",
      "[8,    90] loss: 1.944\n",
      "[8,   120] loss: 1.950\n",
      "accuracy on test set: 59 % \n",
      "61.93939393939394\n",
      "[9,    30] loss: 1.810\n",
      "[9,    60] loss: 1.863\n",
      "[9,    90] loss: 1.814\n",
      "[9,   120] loss: 1.844\n",
      "accuracy on test set: 57 % \n",
      "61.93939393939394\n",
      "[10,    30] loss: 1.690\n",
      "[10,    60] loss: 1.680\n",
      "[10,    90] loss: 1.705\n",
      "[10,   120] loss: 1.777\n",
      "accuracy on test set: 57 % \n",
      "61.93939393939394\n",
      "[11,    30] loss: 1.561\n",
      "[11,    60] loss: 1.584\n",
      "[11,    90] loss: 1.544\n",
      "[11,   120] loss: 1.599\n",
      "accuracy on test set: 51 % \n",
      "61.93939393939394\n",
      "[12,    30] loss: 1.364\n",
      "[12,    60] loss: 1.367\n",
      "[12,    90] loss: 1.428\n",
      "[12,   120] loss: 1.446\n",
      "accuracy on test set: 49 % \n",
      "61.93939393939394\n",
      "[13,    30] loss: 1.146\n",
      "[13,    60] loss: 1.225\n",
      "[13,    90] loss: 1.222\n",
      "[13,   120] loss: 1.236\n",
      "accuracy on test set: 47 % \n",
      "61.93939393939394\n",
      "[14,    30] loss: 0.996\n",
      "[14,    60] loss: 1.001\n",
      "[14,    90] loss: 1.032\n",
      "[14,   120] loss: 1.070\n",
      "accuracy on test set: 44 % \n",
      "61.93939393939394\n",
      "[15,    30] loss: 0.834\n",
      "[15,    60] loss: 0.764\n",
      "[15,    90] loss: 0.869\n",
      "[15,   120] loss: 0.914\n",
      "accuracy on test set: 44 % \n",
      "61.93939393939394\n",
      "[16,    30] loss: 0.671\n",
      "[16,    60] loss: 0.698\n",
      "[16,    90] loss: 0.714\n",
      "[16,   120] loss: 0.755\n",
      "accuracy on test set: 44 % \n",
      "61.93939393939394\n",
      "[17,    30] loss: 0.536\n",
      "[17,    60] loss: 0.595\n",
      "[17,    90] loss: 0.584\n",
      "[17,   120] loss: 0.606\n",
      "accuracy on test set: 41 % \n",
      "61.93939393939394\n",
      "[18,    30] loss: 0.502\n",
      "[18,    60] loss: 0.470\n",
      "[18,    90] loss: 0.544\n",
      "[18,   120] loss: 0.534\n",
      "accuracy on test set: 38 % \n",
      "61.93939393939394\n",
      "[19,    30] loss: 0.427\n",
      "[19,    60] loss: 0.432\n",
      "[19,    90] loss: 0.425\n",
      "[19,   120] loss: 0.495\n",
      "accuracy on test set: 38 % \n",
      "61.93939393939394\n",
      "[20,    30] loss: 0.356\n",
      "[20,    60] loss: 0.378\n",
      "[20,    90] loss: 0.404\n",
      "[20,   120] loss: 0.388\n",
      "accuracy on test set: 37 % \n",
      "61.93939393939394\n",
      "[21,    30] loss: 0.315\n",
      "[21,    60] loss: 0.342\n",
      "[21,    90] loss: 0.386\n",
      "[21,   120] loss: 0.357\n",
      "accuracy on test set: 38 % \n",
      "61.93939393939394\n",
      "[22,    30] loss: 0.296\n",
      "[22,    60] loss: 0.331\n",
      "[22,    90] loss: 0.398\n",
      "[22,   120] loss: 0.342\n",
      "accuracy on test set: 36 % \n",
      "61.93939393939394\n",
      "[23,    30] loss: 0.316\n",
      "[23,    60] loss: 0.298\n",
      "[23,    90] loss: 0.324\n",
      "[23,   120] loss: 0.348\n",
      "accuracy on test set: 38 % \n",
      "61.93939393939394\n",
      "[24,    30] loss: 0.280\n",
      "[24,    60] loss: 0.283\n",
      "[24,    90] loss: 0.270\n",
      "[24,   120] loss: 0.333\n",
      "accuracy on test set: 35 % \n",
      "61.93939393939394\n",
      "[25,    30] loss: 0.258\n",
      "[25,    60] loss: 0.292\n",
      "[25,    90] loss: 0.281\n",
      "[25,   120] loss: 0.270\n",
      "accuracy on test set: 34 % \n",
      "61.93939393939394\n",
      "[26,    30] loss: 0.268\n",
      "[26,    60] loss: 0.268\n",
      "[26,    90] loss: 0.277\n",
      "[26,   120] loss: 0.312\n",
      "accuracy on test set: 37 % \n",
      "61.93939393939394\n",
      "[27,    30] loss: 0.228\n",
      "[27,    60] loss: 0.233\n",
      "[27,    90] loss: 0.259\n",
      "[27,   120] loss: 0.282\n",
      "accuracy on test set: 32 % \n",
      "61.93939393939394\n",
      "[28,    30] loss: 0.216\n",
      "[28,    60] loss: 0.242\n",
      "[28,    90] loss: 0.229\n",
      "[28,   120] loss: 0.300\n",
      "accuracy on test set: 36 % \n",
      "61.93939393939394\n",
      "[29,    30] loss: 0.219\n",
      "[29,    60] loss: 0.194\n",
      "[29,    90] loss: 0.229\n",
      "[29,   120] loss: 0.220\n",
      "accuracy on test set: 34 % \n",
      "61.93939393939394\n",
      "[30,    30] loss: 0.186\n",
      "[30,    60] loss: 0.209\n",
      "[30,    90] loss: 0.262\n",
      "[30,   120] loss: 0.263\n",
      "accuracy on test set: 35 % \n",
      "61.93939393939394\n",
      "[31,    30] loss: 0.236\n",
      "[31,    60] loss: 0.252\n",
      "[31,    90] loss: 0.244\n",
      "[31,   120] loss: 0.302\n",
      "accuracy on test set: 35 % \n",
      "61.93939393939394\n",
      "[32,    30] loss: 0.255\n",
      "[32,    60] loss: 0.247\n",
      "[32,    90] loss: 0.261\n",
      "[32,   120] loss: 0.302\n",
      "accuracy on test set: 35 % \n",
      "61.93939393939394\n",
      "[33,    30] loss: 0.250\n",
      "[33,    60] loss: 0.235\n",
      "[33,    90] loss: 0.246\n",
      "[33,   120] loss: 0.201\n",
      "accuracy on test set: 35 % \n",
      "61.93939393939394\n",
      "[34,    30] loss: 0.173\n",
      "[34,    60] loss: 0.185\n",
      "[34,    90] loss: 0.186\n",
      "[34,   120] loss: 0.185\n",
      "accuracy on test set: 36 % \n",
      "61.93939393939394\n",
      "[35,    30] loss: 0.137\n",
      "[35,    60] loss: 0.150\n",
      "[35,    90] loss: 0.154\n",
      "[35,   120] loss: 0.148\n",
      "accuracy on test set: 35 % \n",
      "61.93939393939394\n",
      "[36,    30] loss: 0.145\n",
      "[36,    60] loss: 0.149\n",
      "[36,    90] loss: 0.145\n",
      "[36,   120] loss: 0.140\n",
      "accuracy on test set: 34 % \n",
      "61.93939393939394\n",
      "[37,    30] loss: 0.157\n",
      "[37,    60] loss: 0.121\n",
      "[37,    90] loss: 0.141\n",
      "[37,   120] loss: 0.145\n",
      "accuracy on test set: 34 % \n",
      "61.93939393939394\n",
      "[38,    30] loss: 0.138\n",
      "[38,    60] loss: 0.128\n",
      "[38,    90] loss: 0.144\n",
      "[38,   120] loss: 0.128\n",
      "accuracy on test set: 33 % \n",
      "61.93939393939394\n",
      "[39,    30] loss: 0.142\n",
      "[39,    60] loss: 0.102\n",
      "[39,    90] loss: 0.148\n",
      "[39,   120] loss: 0.130\n",
      "accuracy on test set: 33 % \n",
      "61.93939393939394\n",
      "[40,    30] loss: 0.148\n",
      "[40,    60] loss: 0.145\n",
      "[40,    90] loss: 0.187\n",
      "[40,   120] loss: 0.216\n",
      "accuracy on test set: 34 % \n",
      "61.93939393939394\n",
      "20.88074040412903\n"
     ]
    }
   ],
   "source": [
    "# Save the Network\n",
    "num_epochs = 5\n",
    "num_classes = 11\n",
    "batch_size = 60\n",
    "learning_rate = 0.001\n",
    "if __name__ == '__main__':\n",
    "    his10_noiseconv = [0]\n",
    "    optim = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(40):\n",
    "        train(epoch)\n",
    "        acc = test()\n",
    "        optim = max(his10_noiseconv)\n",
    "        if acc >= optim:\n",
    "            torch.save(model.state_dict(), \"CNN_signal_0.3.pkl\")\n",
    "        his10_noiseconv.append(acc)\n",
    "        print(optim)\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21992f82",
   "metadata": {},
   "source": [
    "## CNN (Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "497f53f3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Empty network architure\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.type(torch.float32)\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index, :, :, :]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x[:, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdfc97a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7700"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02656281",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 11\n",
    "learning_rate = 0.001  \n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 50, kernel_size = (1, 8), stride = (1, 1), padding = (0, 2)),\n",
    "          nn.Dropout2d(),\n",
    "          nn.BatchNorm2d(50),#正则化\n",
    "          nn.ReLU()\n",
    "          #nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 1))\n",
    "         )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(50, 50, kernel_size=(2, 8), stride=(1, 1), padding=(0, 2)),\n",
    "            nn.Dropout2d(),\n",
    "            nn.BatchNorm2d(50),\n",
    "            nn.ReLU()\n",
    "#             nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 1))\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(6100, 256)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        temp = self.layer3(out)\n",
    "        out = self.layer4(temp)\n",
    "        return out, temp\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "new_model = model                                                   # 调用模型Model\n",
    "new_model.load_state_dict(torch.load(\"./CNN_signal_0.3.pkl\"))    # 加载模型参数     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e89d13",
   "metadata": {},
   "source": [
    "### for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d35ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extractor for train\n",
    "def ch_gen(model, train_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images#.to(device)\n",
    "            labels = labels#.to(device)\n",
    "            outputs, temp = model(images)\n",
    "            \n",
    "    return temp\n",
    "\n",
    "def character_extract():\n",
    "    train_dataset = MyDataset(train_x, y_train)\n",
    "    batch_size = train_x.shape[0]\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=train_x.shape[0])                                            \n",
    "    \n",
    "\n",
    "    character = ch_gen(new_model, train_loader)\n",
    "    character = character.cpu().detach().numpy()\n",
    "    return character, y_train.cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc427843",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_3_logits_before, signal_3_y = character_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d74d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = pd.DataFrame(signal_3_logits_before)\n",
    "save.to_csv(\"train_noise_logits_before.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9725caa8",
   "metadata": {},
   "source": [
    "### for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec67c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extractor for train\n",
    "def ch_gen_test(model, train_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images#.to(device)\n",
    "            labels = labels#.to(device)\n",
    "            outputs, temp = model(images)\n",
    "            \n",
    "    return temp\n",
    "\n",
    "def character_extract_test():\n",
    "    train_dataset = MyDataset(test_x, y_test)\n",
    "    batch_size = test_x.shape[0]\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=test_x.shape[0])                                            \n",
    "    \n",
    "\n",
    "    character = ch_gen_test(new_model, train_loader)\n",
    "    character = character.cpu().detach().numpy()\n",
    "    return character\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "254f29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_3_logits_before_test = character_extract_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc41712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test = pd.DataFrame(signal_3_logits_before_test)\n",
    "save_test.to_csv(\"test_noise_logits_before.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7a6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shiro",
   "language": "python",
   "name": "shiro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
